{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deep learning Bag-of-Words model for predicting movie review sentiment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This project aims to create an algorithm that gives a binary classification to a movie review, indicating a positive or a negative sentiment."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The  data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The data used for this project is obtained from Kaggle, it is the [IMDB Movie Reviews Dataset](https://www.kaggle.com/iarunava/imdb-movie-reviews-dataset). This dataset contains movie reviews along with their associated binary sentiment polarity labels. <br>\n",
    "The core dataset contains 50,000 reviews split evenly into 25k train\n",
    "and 25k test sets. The overall distribution of labels is balanced (25k\n",
    "pos and 25k neg). <br>\n",
    "In the entire collection, no more than 30 reviews are allowed for any\n",
    "given movie because reviews for the same movie tend to have correlated\n",
    "ratings. Further, the train and test sets contain a disjoint set of\n",
    "movies, so no significant performance is obtained by memorizing\n",
    "movie-unique terms and their associated with observed labels.  In the\n",
    "labeled train/test sets, a negative review has a score <= 4 out of 10,\n",
    "and a positive review has a score >= 7 out of 10. Thus reviews with\n",
    "more neutral ratings are not included in the train/test sets. <br>\n",
    "In addition to the review text files, we have a text file  with all the words used in the entirety of the reviews. We call this file the vocabulary of the reviews."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "PATH_TO_DATA = os.path.dirname(os.path.abspath(\"__file__\")) + \"/IMDB_reviews\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "import random\n",
    "\n",
    "# list of paths to the reviews\n",
    "paths_to_reviews_test = glob.glob(PATH_TO_DATA + \"/test/neg/*.txt\") + glob.glob(PATH_TO_DATA + \"/test/pos/*.txt\")\n",
    "random.shuffle(paths_to_reviews_test)\n",
    "paths_to_reviews_train = glob.glob(PATH_TO_DATA + \"/train/neg/*.txt\") + glob.glob(PATH_TO_DATA + \"/train/pos/*.txt\")\n",
    "random.shuffle(paths_to_reviews_train)\n",
    "\n",
    "# we create our train and test set (list of (review, sentiment) with sentiment in {0, 1}).\n",
    "train_set = []\n",
    "for review_path in paths_to_reviews_train:\n",
    "    filename = os.path.basename(review_path)\n",
    "    rating = os.path.splitext(filename)[0][-1]\n",
    "    label = 0 if int(rating) <= 4 else 1\n",
    "    with open(review_path, 'rb') as f:\n",
    "        review = str(f.read())\n",
    "    train_set.append((review, label))\n",
    "\n",
    "test_set = []\n",
    "for review_path in paths_to_reviews_test:\n",
    "    filename = os.path.basename(review_path)\n",
    "    rating = os.path.splitext(filename)[0][-1]\n",
    "    label = 0 if int(rating) <= 4 else 1\n",
    "    with open(review_path, 'rb') as f:\n",
    "        review = str(f.read())\n",
    "    test_set.append((review, label))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Methodology"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will use a common approach which is called \"Bag of Words\". Let's take a simple example to highlight how a bag-of-word model works : <br>\n",
    "Let's say we have two sentences : \n",
    "* sentence 1 : \"Allons enfants de la Patrie\"\n",
    "* sentence 2 : \"Les enfants, allons à la piscine\" </ul>\n",
    "\n",
    "Given that the vocabulary from the two sentences is `{allons, les, enfants, de, la, à, piscine, patrie}`, we simply construct a vector based on the number of occurrences of each word in a sentence. The sequence of words within a sentence does not matter.<br>\n",
    "\n",
    "|      Sentence                   |allons | les | enfants | de | la | à | piscine | patrie |\n",
    "| :------------                   | :---: | :-: | :--:  | :---:|:--:|:-:|:------:| :-----: |\n",
    "| Allons enfants de la Patrie     |  1    |  0 | 1    | 1 | 1| 0 | 0 | 1 |\n",
    "| Les enfants, allons à la piscine |  1    |  1 | 1    | 0 | 1| 1 | 1 | 0 |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each sentence can thus be transformed into a vector, with the length of the vector being the number of words in the vocabulary. During classification, the model will then possibly learn that higher occurrences of certain words are more likely to lead to a particular prediction."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Vectorization of the reviews"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The vocabulary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First we need to choose the vocabulary that defines the vector space in which we will vectorize the reviews. We could use all the words that appears at least once in the whole dataset of reviews. However the dimension of our vector space would be huge. We will study the vocabulary to try to diminish this dimension."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Listing the whole vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "dico_vocab = {}\n",
    "for review, label in (train_set):\n",
    "    already_seen = []\n",
    "    for word in review.split(\" \"):\n",
    "        word = word.lower()\n",
    "        if len(word) > 1 and word.isalpha() and not word in already_seen:\n",
    "            dico_vocab[word] = dico_vocab.get(word, 0) + 1\n",
    "            already_seen.append(word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of different words in all the reviews : 60615\n"
     ]
    }
   ],
   "source": [
    "print(f\"number of different words in all the reviews : {len(dico_vocab)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('is', 22310), ('to', 23438), ('of', 23702), ('and', 24069), ('the', 24759)]"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocabulary = list(dico_vocab.items())\n",
    "vocabulary.sort(key=lambda x: x[1])\n",
    "vocabulary[60610:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Reducing the size of the vocabulary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We drop stopwords from the vocabulary (ie 'the', 'or', 'and'...)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "60521"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.corpus import stopwords\n",
    "sw = stopwords.words(\"english\")\n",
    "\n",
    "for i, (word, occ) in enumerate(vocabulary):\n",
    "    if word in sw:\n",
    "        vocabulary.pop(i)\n",
    "\n",
    "len(vocabulary)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we choose to only keep the 5,000 most frequent words. It is arbitrary but if the score we'll get is not satisfactory we'll change this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocabulary_reduced = vocabulary[len(vocabulary) - 5000:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we stock the vocabulary in a list (we drop the number of occurences)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab = []\n",
    "for (word, occ) in vocabulary_reduced:\n",
    "    vocab.append(word)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Text vectorization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have defined a vocabulary, we are able to transform the reviews in vectors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "def vectorizer(text, vocabulary):\n",
    "    vector = [0 for _ in vocabulary]\n",
    "    for word in text.split(\" \"):\n",
    "        word = word.lower()\n",
    "        if word in vocabulary:\n",
    "            vector[vocabulary.index(word)] += 1\n",
    "    return(vector)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using the function `vectorizer` we vectorize the review both in the test and train set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vectorizing review 0/25,000 (train set)\n",
      "vectorizing review 1000/25,000 (train set)\n",
      "vectorizing review 2000/25,000 (train set)\n",
      "vectorizing review 3000/25,000 (train set)\n",
      "vectorizing review 4000/25,000 (train set)\n",
      "vectorizing review 5000/25,000 (train set)\n",
      "vectorizing review 6000/25,000 (train set)\n",
      "vectorizing review 7000/25,000 (train set)\n",
      "vectorizing review 8000/25,000 (train set)\n",
      "vectorizing review 9000/25,000 (train set)\n",
      "vectorizing review 10000/25,000 (train set)\n",
      "vectorizing review 11000/25,000 (train set)\n",
      "vectorizing review 12000/25,000 (train set)\n",
      "vectorizing review 13000/25,000 (train set)\n",
      "vectorizing review 14000/25,000 (train set)\n",
      "vectorizing review 15000/25,000 (train set)\n",
      "vectorizing review 16000/25,000 (train set)\n",
      "vectorizing review 17000/25,000 (train set)\n",
      "vectorizing review 18000/25,000 (train set)\n",
      "vectorizing review 19000/25,000 (train set)\n",
      "vectorizing review 20000/25,000 (train set)\n",
      "vectorizing review 21000/25,000 (train set)\n",
      "vectorizing review 22000/25,000 (train set)\n",
      "vectorizing review 23000/25,000 (train set)\n",
      "vectorizing review 24000/25,000 (train set)\n",
      "vectorizing review 0/25,000 (test set)\n",
      "vectorizing review 1000/25,000 (test set)\n",
      "vectorizing review 2000/25,000 (test set)\n",
      "vectorizing review 3000/25,000 (test set)\n",
      "vectorizing review 4000/25,000 (test set)\n",
      "vectorizing review 5000/25,000 (test set)\n",
      "vectorizing review 6000/25,000 (test set)\n",
      "vectorizing review 7000/25,000 (test set)\n",
      "vectorizing review 8000/25,000 (test set)\n",
      "vectorizing review 9000/25,000 (test set)\n",
      "vectorizing review 10000/25,000 (test set)\n",
      "vectorizing review 11000/25,000 (test set)\n",
      "vectorizing review 12000/25,000 (test set)\n",
      "vectorizing review 13000/25,000 (test set)\n",
      "vectorizing review 14000/25,000 (test set)\n",
      "vectorizing review 15000/25,000 (test set)\n",
      "vectorizing review 16000/25,000 (test set)\n",
      "vectorizing review 17000/25,000 (test set)\n",
      "vectorizing review 18000/25,000 (test set)\n",
      "vectorizing review 19000/25,000 (test set)\n",
      "vectorizing review 20000/25,000 (test set)\n",
      "vectorizing review 21000/25,000 (test set)\n",
      "vectorizing review 22000/25,000 (test set)\n",
      "vectorizing review 23000/25,000 (test set)\n",
      "vectorizing review 24000/25,000 (test set)\n"
     ]
    }
   ],
   "source": [
    "x_train, x_test = [], []\n",
    "y_train, y_test = [], []\n",
    "\n",
    "for i, (review, label) in enumerate(train_set):\n",
    "    x_train.append(vectorizer(review, vocab))\n",
    "    y_train.append(label)\n",
    "    if i == 0 or i%1000 == 0:\n",
    "        print(f\"vectorizing review {i}/25,000 (train set)\")\n",
    "\n",
    "for i, (review, label) in enumerate(test_set):\n",
    "    x_test.append(vectorizer(review, vocab))\n",
    "    y_test.append(label)\n",
    "    if i == 0 or i%1000 == 0:\n",
    "        print(f\"vectorizing review {i}/25,000 (test set)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training and testing our model "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will use a Logistic Regression."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/Paul-Noel/anaconda3/lib/python3.6/site-packages/sklearn/linear_model/sag.py:334: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "          intercept_scaling=1, max_iter=100, multi_class='warn',\n",
       "          n_jobs=None, penalty='l2', random_state=0, solver='sag',\n",
       "          tol=0.0001, verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf_LR = LogisticRegression(random_state=0, solver='sag')\n",
    "clf_LR.fit(x_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_test = np.array(clf_LR.predict(x_test))\n",
    "accuracy = np.mean(pred_test == np.array(y_test).T[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.79488"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
